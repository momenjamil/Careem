{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Consolidated Files for Customer and Captain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:88: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta \n",
    "import sqlalchemy  \n",
    "from pyhive import presto\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "\n",
    "# Read raw data of data sources in Pandas datafromes.\n",
    "df_Customer = pd.read_csv ('C:/Users/momen.jamil/Desktop/Careem/Work/Lovish/FCR/Week 48/ZD/Customer_FCR_Raw_data.csv', keep_default_na = False)\n",
    "df_Voice = pd.read_csv ('C:/Users/momen.jamil/Desktop/Careem/Work/Lovish/FCR/Week 48/Voice/Disposition_for_LLS_customer_2019_12_01.csv', keep_default_na = False)\n",
    "df_Chat = pd.read_excel ('C:/Users/momen.jamil/Desktop/Careem/Work/Lovish/FCR/Week 48/Chat/Session_time_Chat_Concurrency_2019_12_02.xlsx', keep_default_na = False)\n",
    "df_Captain = pd.read_csv ('C:/Users/momen.jamil/Desktop/Careem/Work/Lovish/FCR/Week 48/ZD/Captain_FCR_Raw_data.csv', keep_default_na = False)\n",
    "\n",
    "# Read the mapping file from presto.\n",
    "presto_02_fin = sqlalchemy.create_engine('presto://hidden')\n",
    "query = open('C:/Users/momen.jamil/Desktop/Careem/Work/Lovish/FCR/Testing_Python//mapping file.txt', 'r').read()\n",
    "df_Mapping_Complete = pd.read_sql(con=presto_02_fin, sql=query) \n",
    "\n",
    "# Choose only the required columns from mapping file along with renaming queue column name.\n",
    "df_Mapping_Complete = df_Mapping_Complete.rename(columns = {\"resource_name\":\"queue (Ticket Group)\"})\n",
    "df_Mapping_Needed  =  df_Mapping_Complete[['queue (Ticket Group)','domain','media','captain_consumer','for_lls_dashboard']]\n",
    "\n",
    "\n",
    "# Renaming Customer data source Columns to make columns names across all data sources consistent.  \n",
    "df_Customer = df_Customer.rename(columns = {\"Date (Ticket Created)\": \"created\", \"Phone number.1\" : \"phonenumber\", \"User ID\" : \"userid\", \n",
    "                              \"Ticket Id\" : \"callid (ticket id)\", \"User External ID\" : \"Captain_ID (User external ID)\",\n",
    "                              \"Reason (Quality/GO/Billing)\" : \"Disposition\", \"Ticket Group\" : \"queue (Ticket Group)\",\n",
    "                               \"Ticket Assignee\" : \"agentname (Ticket Assignee)\",\"Country\" : \"country\" , \"Booking ID\" : \"bookingid\"})\n",
    "\n",
    "del df_Customer['Phone number'] # deleting the second \"Phone number\" column.\n",
    "\n",
    "\n",
    "df_Customer['data_Source'] = 'Zendesk' # Create 'data_Source' column and asign 'Zendesk' to Customer data frame. \n",
    "df_Customer['Start_Time'] = pd.to_datetime(df_Customer['created'])\n",
    "df_Customer['Start_Time'] = df_Customer['Start_Time']+ timedelta(hours = 23, minutes = 59) # 23:59 Hypothesis for no time contacts.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Renaming Captain data source Columns to make columns names across all data sources consistent.\n",
    "df_Captain = df_Captain.rename(columns = {\"Date (Ticket Created)\": \"created\", \"Phone number.1\" : \"phonenumber\", \"User ID\" : \"userid\", \n",
    "                              \"Ticket Id\" : \"callid (ticket id)\", \"User External ID\" : \"Captain_ID (User external ID)\",\n",
    "                              \"Captain Support Reason\" : \"Disposition\", \"Ticket Group\" : \"queue (Ticket Group)\",\n",
    "                              \"Ticket Assignee\" : \"agentname (Ticket Assignee)\",\"Country\" : \"country\" , \"Booking ID\" : \"bookingid\"})\n",
    "\n",
    "del df_Captain['Phone number']\n",
    "\n",
    "df_Captain['data_Source'] = 'Zendesk'\n",
    "df_Captain['Start_Time'] = pd.to_datetime(df_Captain['created'])\n",
    "df_Captain['Start_Time'] = df_Captain['Start_Time']+ timedelta(hours = 23, minutes = 59)\n",
    "\n",
    "\n",
    "\n",
    "df_Voice['date1'] = df_Voice['date1'] + \" \" + df_Voice['time'] # Merge date and time columns in one column.\n",
    "del df_Voice['time']\n",
    "\n",
    "# Renaming Voice data source Columns to make columns names across all data sources consistent.  \n",
    "df_Voice = df_Voice.rename(columns = {\"date1\": \"created\", \"PhoneNumber\" : \"phonenumber\", \"user_id\" : \"userid\", \n",
    "                              \"callid\" : \"callid (ticket id)\", \"captain_id\" : \"Captain_ID (User external ID)\",\n",
    "                              \"new_disposition\" : \"Disposition\", \"queue\" : \"queue (Ticket Group)\",\n",
    "                              \"agent\" : \"agentname (Ticket Assignee)\", \"booking_id\" : \"bookingid\", \"aht\" : \"AHT\"})\n",
    "\n",
    "df_Voice['data_Source'] = 'Genesys' # Genesys is the source for voice calls data so we assgin Genesys data source to Voice DF. \n",
    "df_Voice['created'] = pd.to_datetime(df_Voice['created']) # Changing Created Column to Datetime type.\n",
    "df_Voice['Start_Time'] = df_Voice['created']\n",
    "\n",
    "# Required actions in Chat data source.\n",
    "df_Chat = df_Chat.rename(columns = { \n",
    "                              \"callid\" : \"callid (ticket id)\",\n",
    "                              \"disp\" : \"Disposition\", \"queue\" : \"queue (Ticket Group)\",\n",
    "                              \"agentname\" : \"agentname (Ticket Assignee)\"})\n",
    "\n",
    "\n",
    "\n",
    "df_Chat.drop(df_Chat[df_Chat['queue (Ticket Group)'] == 'Test_VQ'].index, inplace = True) # Excluding Test queue from our calculations.\n",
    "df_Chat['data_Source'] = 'Genesys' # Genesys is the source for Chat Contacts data so we assgin Genesys data source to Voice DF.\n",
    "df_Chat['created'] = pd.to_datetime(df_Chat['created'])\n",
    "df_Chat['Start_Time'] = df_Chat['created']\n",
    "\n",
    "\n",
    "# Consolidated file creation : Merge all data fromes together. \n",
    "df_Consolidated = pd.concat([df_Voice, df_Chat, df_Customer, df_Captain],ignore_index=True)\n",
    "\n",
    "# Merging the Consolidated file with the mapping file based in queue column.\n",
    "df_Final = df_Consolidated.merge(df_Mapping_Needed, on='queue (Ticket Group)')\n",
    "\n",
    "df_Final = df_Final[df_Final['domain'] == 'Ride Hailing']\n",
    "# Fixing an issue with the mapping file considering \"CHAT_RIDE_UAE_AR_CUS_VQ\" queue as Email type, we replace it with \"Chat\".\n",
    "Mask_UAE_Queue = df_Final['queue (Ticket Group)'] == 'CHAT_RIDE_UAE_AR_CUS_VQ'\n",
    "df_Final['media'][Mask_UAE_Queue == True] = 'Chat'\n",
    "\n",
    "# Changing media type string from \"zendesk_nv\" to \"Non_Voice\", beneficial in upcoming queries.\n",
    "Mask_zendesk_nv = df_Final['media'] == 'zendesk_nv'\n",
    "df_Final['media'][Mask_zendesk_nv == True] = 'Non_Voice'\n",
    "\n",
    "# Create Date and Timestamp_Form Columns. \n",
    "df_Final['Date'] = df_Final['Start_Time'].dt.date\n",
    "df_Final['Timestamp_Form'] = df_Final['Start_Time'].dt.time\n",
    "\n",
    "# Create Customer and Captain separate files.\n",
    "df_Final_Customer = df_Final.loc[df_Final['captain_consumer'] == 'Customer']\n",
    "df_Final_Captain = df_Final.loc[df_Final['captain_consumer'] == 'Captain']\n",
    "\n",
    "# Asending sorting of datafromes based on \"Start_Time\" column.\n",
    "df_Final_Customer = df_Final_Customer.sort_values('Start_Time')\n",
    "df_Final_Captain = df_Final_Captain.sort_values('Start_Time')\n",
    "\n",
    "\n",
    "\n",
    "# coming lines can be used for expoerting Customer and Captain raw data files.\n",
    "#--df_array[0].to_excel('C:/Users/momen.jamil/Desktop/Careem/Work/Lovish/FCR/Week 48/Final_Consolidated_Customer_File.xlsx', index =False , sheet_name='Sheet1')\n",
    "#--df_array[1].to_excel('C:/Users/momen.jamil/Desktop/Careem/Work/Lovish/FCR/Week 48/Final_Consolidated_Captain_File.xlsx', index =False , sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n",
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Create an array contains Customer and Captain dataframes as we need to do the same actions on both of them separately.\n",
    "df_array = [df_Final_Customer, df_Final_Captain] \n",
    "\n",
    "for i in range(0,2):\n",
    "    df_array[i] = df_array[i].reset_index() # Resetting Indexes of both data frames this's will help in looping through rows.\n",
    "\n",
    "    # Replace blanks, '-' and 'null' string with Nan as we want to deal with all of them as Nans.\n",
    "    Nan_mask = (df_array[i]['bookingid'] == '') | (df_array[i]['bookingid'] == 'null')\n",
    "    df_array[i]['bookingid'][Nan_mask == True] = np.nan \n",
    "    Nan_mask = (df_array[i]['userid'] == '') | (df_array[i]['userid'] == '-') | (df_array[i]['userid'] == 'null')\n",
    "    df_array[i]['userid'][Nan_mask == True] = np.nan\n",
    "    Nan_mask = (df_array[i]['Disposition'] == 'N/A') | (df_array[i]['Disposition'] == '')\n",
    "    df_array[i]['Disposition'][Nan_mask == True] = np.nan\n",
    "    Nan_mask = (df_array[i]['# Reopens'] == '')\n",
    "    df_array[i]['# Reopens'][Nan_mask == True] = np.nan\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disposition Flag Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Grid Dictionary for Dispositions.\n",
    "grid_dict={}\n",
    "grid_dict['Others____Internal transfer']='Transfer'\n",
    "grid_dict['Call Dropped']='Dropped/Silent Calls'\n",
    "grid_dict['Operations related - Dropped call']='Dropped/Silent Calls'\n",
    "grid_dict['Operations related - Dropped calls']='Dropped/Silent Calls'\n",
    "grid_dict['Others____Dropped\\\\\\\\ Silent call']='Dropped/Silent Calls'\n",
    "grid_dict['Operations related - Silent Calls']='Dropped/Silent Calls'\n",
    "grid_dict['Operations related - Silent Call']='Dropped/Silent Calls'\n",
    "\n",
    "for i in range(0,2):\n",
    "        df_array[i]['Disposition Flags'] = df_array[i]['Disposition'].map(grid_dict) # map each disposition with its flag.\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New_Booking_ID Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# looping through each dataframe in df_array (df_Final_Customer and df_Final_Captain) and od exactly the same actions.\n",
    "\n",
    "for j in range(0,2): \n",
    "    df_array[j]['New_Booking_ID'] = np.nan  # Create an empty New_Booking_ID column.\n",
    "    \n",
    "    # Create pandas series called mask to check in each row if userid matched previous row's userid along with Nan Bookingid. \n",
    "    # Mask is an efficient way to update pandas series (Column) without looping through all cells.\n",
    "    mask = df_array[j]['userid'].eq(df_array[j]['userid'].shift(1)) & pd.isnull(df_array[j]['bookingid'])\n",
    "\n",
    "\n",
    "    # Filling out 'New_Booking_ID' Column based on mask value.\n",
    "    df_array[j]['New_Booking_ID'][mask == False] = df_array[j]['bookingid']\n",
    "    df_array[j]['New_Booking_ID'][mask == True] = df_array[j]['New_Booking_ID'].shift(1)\n",
    "\n",
    "\n",
    "    # Mask is quite efficient however it does the action only one time, in cases where there're a lot of blanks under each other\n",
    "    # mask will only update the first blank cell to fix it we'll only iterate on blanks cells. \n",
    "    \n",
    "    Null_index= df_array[j]['New_Booking_ID'][((pd.isnull(df_array[j]['New_Booking_ID'])) == True)].index # Finding indexes of blanks cells in \"New_Booking_ID\". \n",
    "    for i in Null_index:\n",
    "        if i !=0  and (df_array[j].loc[i,'userid'] == df_array[j].loc[i-1,'userid']) or (pd.isnull(df_array[j].loc[i,'userid']) and pd.isnull(df_array[j].loc[i-1,'userid']) == True):\n",
    "            if pd.isnull(df_array[j].loc[i,'bookingid']):\n",
    "                df_array[j].loc[i,'New_Booking_ID'] = df_array[j].loc[i-1,'New_Booking_ID']\n",
    "            else:\n",
    "\n",
    "                df_array[j].loc[i,'New_Booking_ID']= df_array[j].loc[i, 'bookingid']       \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User ID availability Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,2):\n",
    "    # create 'User_id_avail' will equal to 1 if userid in this row exist otherwise it'll equal to 0\n",
    "    df_array[j]['User_id_avail'] = pd.notnull(df_array[j]['userid']).apply(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min_Threshold_Time Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Threshold dictionary with start (beginning of TH period) as the first key and end as the second one.\n",
    "\n",
    "grid_dict_TH={}\n",
    "grid_dict_TH[\"Voice\"]=[0, 24]  \n",
    "grid_dict_TH[\"Non_Voice\"]= [ 0 , 24]\n",
    "grid_dict_TH[\"Chat\"]= [2, 24]\n",
    "\n",
    "for j in range(0,2):\n",
    "    # Creat an empty Min_Threshold_Time column\n",
    "    df_array[j]['Min_Threshold_Time'] = np.nan\n",
    "    \n",
    "    # Change type of column to be daatetime.\n",
    "    df_array[j]['Min_Threshold_Time'] = pd.to_datetime(df_array[j]['Min_Threshold_Time']) \n",
    "    \n",
    "    # Create a list contains TH_dictionary values (two series one for start and another one for End)\n",
    "    TH_dictionary_Values = pd.DataFrame(df_array[j][\"media\"].map(grid_dict_TH).values.tolist())\n",
    "    \n",
    "    # create pandas series contains only start values.\n",
    "    Start_Series = TH_dictionary_Values.iloc[:,0]\n",
    "    \n",
    "    \n",
    "    # Take the generic formula of calculating 'Min_Threshold_Time' column = Start_Time + Start_Series(begainning of TH) \n",
    "    df_array[j]['Min_Threshold_Time']= df_array[j]['Start_Time'] + pd.to_timedelta(Start_Series, 'h')\n",
    "\n",
    "    # Create a mask equals to True when userid of a row matched previous row userid.\n",
    "    mask_User = df_array[j]['userid'].eq(df_array[j]['userid'].shift(1))\n",
    "\n",
    "     # Create pandas series contains all indexes of cells where mask_User is True.\n",
    "    Condition_index = df_array[j]['Min_Threshold_Time'][mask_User == True].index \n",
    "    \n",
    "    # Looping through True mask_User indexes and update values if their rows matched the right conditions.\n",
    "    for i in Condition_index:\n",
    "        if  i!=0 and df_array[j].loc[i,'userid'] == df_array[j].loc[i-1,'userid'] and df_array[j].loc[i,'Start_Time'] < df_array[j].loc[i-1,'Min_Threshold_Time'] :\n",
    "             if (df_array[j].loc[i-1, 'Min_Threshold_Time'] - df_array[j].loc[i, 'Start_Time']) < timedelta(hours = grid_dict_TH[ df_array[j].loc[i, \"media\"]][0]) :\n",
    "                    df_array[j].loc[i, 'Min_Threshold_Time']= df_array[j].loc[i-1, 'Min_Threshold_Time']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End_Time Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,2):\n",
    "    df_array[j]['End_Time'] = np.nan\n",
    "    df_array[j]['End_Time'] = pd.to_datetime(df_array[j]['End_Time'])\n",
    "    \n",
    "    # create pandas series contains only End values.\n",
    "    End_Series = TH_dictionary_Values.iloc[:,1]\n",
    "   \n",
    "\n",
    "    # Mask_User creation.\n",
    "    mask_User = df_array[j]['userid'].eq(df_array[j]['userid'].shift(1)) \n",
    "\n",
    "    \n",
    "    # Take the generic formula of calculating 'End_Time' column = Start_Time + End_Series.\n",
    "    df_array[j]['End_Time']= df_array[j]['Start_Time'] + pd.to_timedelta(End_Series, 'h')\n",
    "\n",
    "  \n",
    "    # Looping through True mask_User indexes and update values if their rows matched the right conditions.\n",
    "    Condition_index = df_array[j]['End_Time'][mask_User == True].index \n",
    "    for i in Condition_index:\n",
    "        if  i!=0 and (df_array[j].loc[i,'userid'] == df_array[j].loc[i-1,'userid'] or pd.isnull(df_array[j].loc[i,'userid']) and pd.isnull(df_array[j].loc[i-1,'userid'])) and (df_array[j].loc[i,'Start_Time'] < df_array[j].loc[i-1,'End_Time']) :\n",
    "            if (df_array[j].loc[i-1, 'End_Time'] - df_array[j].loc[i, 'Start_Time']) < timedelta(hours = grid_dict_TH[ df_array[j].loc[i, \"media\"]][1]) :\n",
    "                   df_array[j].loc[i, 'End_Time']= df_array[j].loc[i-1, 'End_Time']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple_Contacts Column creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,2):\n",
    "    # create Start_Time pandas series\n",
    "    Start_Time = pd.Series(df_array[j]['Start_Time'])\n",
    "    # Pandas deals with nans as nan not equal to nan we need all nans to equal each other.\n",
    "    mask = (df_array[j]['userid'].eq(df_array[j]['userid'].shift(1)) | (pd.isnull(df_array[j]['userid']) & pd.isnull(df_array[j]['userid'].shift(1))) ) & (Start_Time < df_array[j]['End_Time'].shift(1)) & (df_array[j]['New_Booking_ID'].eq(df_array[j]['New_Booking_ID'].shift(1)) | (pd.isnull(df_array[j]['New_Booking_ID']) & pd.isnull(df_array[j]['New_Booking_ID'].shift(1)))) \n",
    "\n",
    "    df_array[j]['Multiple_Contacts'] = mask.apply(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multiple_Contacts (Thres Adjusted) Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for j in range(0,2):\n",
    "    Start_Time = pd.Series(df_array[j]['Start_Time'])\n",
    "    df_array[j]['Multiple_Contacts (Thres Adjusted)'] = np.nan\n",
    "    mask = (df_array[j]['media'].eq(df_array[j]['media'].shift(1))) & (Start_Time < df_array[j]['Min_Threshold_Time'].shift(1)) & df_array[j]['Multiple_Contacts']==1\n",
    "    df_array[j]['Multiple_Contacts (Thres Adjusted)'][mask == True] = 0\n",
    "    df_array[j]['Multiple_Contacts (Thres Adjusted)'][mask == False] = df_array[j]['Multiple_Contacts']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disposition_Category Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\momen.jamil\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for j in range(0,2):\n",
    "    df_array[j]['Disposition_Category'] = np.nan\n",
    "#     mask = (df_array[j]['userid'] != df_array[j]['userid'].shift(1)) & (pd.notnull(df_array[j]['userid'])) & (pd.notnull(df_array[j]['userid'].shift(1))) & (pd.isnull(df_array[j]['Disposition Flags']) == False)  & (df_array[j]['Test_Multiple_Contacts'].shift(-1)!=1)\n",
    "    mask = (df_array[j]['userid'].fillna(0) != (df_array[j]['userid'].shift(1)).fillna(0)) & (pd.isnull(df_array[j]['Disposition Flags']) == False)  & (df_array[j]['Multiple_Contacts'].shift(-1)!=1)\n",
    "\n",
    "    df_array[j]['Disposition_Category'][mask == False] = np.nan\n",
    "    df_array[j]['Disposition_Category'][mask == True] = \"Single Contact -\" + df_array[j]['Disposition Flags']\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZD_Category Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,2):\n",
    "\n",
    "    df_array[j]['ZD_Category'] = np.nan\n",
    "\n",
    "    Reopen_mask = (df_array[j]['data_Source'] == 'Zendesk') & ((df_array[j]['# Tickets Reopened']==1) | (df_array[j]['# Reopens'] >= 1))\n",
    "    Hold_mask = (df_array[j]['data_Source'] == 'Zendesk') & ((df_array[j]['# On Hold Tickets'] == 1) | (df_array[j]['Ticket Status'] == \"Hold\")) \n",
    "    Pending_mask = (df_array[j]['data_Source'] == 'Zendesk') & ((df_array[j]['# Pending Tickets'] == 1) | (df_array[j]['Ticket Status'] == \"Pending\")) \n",
    "    Solved_mask = (df_array[j]['data_Source'] == 'Zendesk') & ((df_array[j]['# Tickets Solved'] == 1) | (df_array[j]['# Tickets Solved (but not closed)'] == 1) | (df_array[j]['# Tickets Solved From Hold - Pankaj'] == 1) | (df_array[j]['# Tickets Solved With English Tag'] == 1) | (df_array[j]['Ticket Status'] == \"Solved\")) \n",
    "    Not_Worked_mask = (df_array[j]['data_Source'] == 'Zendesk') & (pd.isnull(df_array[j]['Disposition'])== True)\n",
    "\n",
    "    df_array[j].loc[Reopen_mask == True, 'ZD_Category'] =\"2_Zendesk_Reopen\"\n",
    "    df_array[j].loc[(Reopen_mask == False) & (Hold_mask == True), 'ZD_Category'] =\"3_Zendesk_On_Hold\"\n",
    "    df_array[j].loc[(Reopen_mask == False) & (Hold_mask == False) & (Pending_mask == True), 'ZD_Category'] = \"4_Zendesk_Pending\"\n",
    "    df_array[j].loc[(Reopen_mask == False) & (Hold_mask == False) & (Pending_mask == False) & (Solved_mask == True), 'ZD_Category'] = \"5_Zendesk_Solved\" \n",
    "    df_array[j].loc[(Reopen_mask == False) & (Hold_mask == False) & (Pending_mask == False) & (Solved_mask == False) & (Not_Worked_mask == True), 'ZD_Category'] = \"6_Zendesk_Not_Worked\"\n",
    "    df_array[j].loc[(Reopen_mask == False) & (Hold_mask == False) & (Pending_mask == False) & (Solved_mask == False) & (Not_Worked_mask == False) & (df_array[j]['data_Source'] == 'Zendesk'), 'ZD_Category'] = '7_Zendesk_Worked_New_Open'\n",
    "    df_array[j].loc[(Reopen_mask == False) & (Hold_mask == False) & (Pending_mask == False) & (Solved_mask == False) & (Not_Worked_mask == False) & (df_array[j]['data_Source'] != 'Zendesk'), 'ZD_Category'] = 'Live_Channels'\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "                 \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final_Category Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,2):\n",
    "    Mask_Multiple = (df_array[j]['Multiple_Contacts (Thres Adjusted)'] ==1) | (df_array[j]['Multiple_Contacts (Thres Adjusted)'].shift(-1)==1)\n",
    "    Mask_Trasfered_Dropped = pd.notnull(df_array[j]['Disposition_Category'])\n",
    "    Mask_ZD_Category = df_array[j]['data_Source'] == 'Zendesk'\n",
    "\n",
    "    df_array[j].loc[Mask_Multiple == True, 'Final_Category'] = \"0_Multiple_Contacts\"\n",
    "    df_array[j].loc[(Mask_Multiple == False) & (Mask_Trasfered_Dropped == True) , 'Final_Category'] = \"1_Transfers_Dropped_Silent\"\n",
    "    df_array[j].loc[(Mask_Multiple == False) & (Mask_Trasfered_Dropped == False) & (Mask_ZD_Category == True)  , 'Final_Category'] = df_array[j]['ZD_Category']\n",
    "    df_array[j].loc[(Mask_Multiple == False) & (Mask_Trasfered_Dropped == False )& (Mask_ZD_Category == False)  , 'Final_Category'] = '8_Live_Channel_Single_Contact'\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dict_Consider={}\n",
    "grid_dict_Consider[\"0_Multiple_Contacts\"]=['2nd One Reopen', 'Within threshold to be excluded'] \n",
    "grid_dict_Consider[\"1_Transfers_Dropped_Silent\"]=[1, 1] \n",
    "grid_dict_Consider[\"2_Zendesk_Reopen\"]=[1, 1] \n",
    "grid_dict_Consider[\"3_Zendesk_On_Hold\"]=[0, 0] \n",
    "grid_dict_Consider[\"4_Zendesk_Pending\"]=[1, 1]\n",
    "grid_dict_Consider[\"5_Zendesk_Solved\"]=[0, 1]\n",
    "grid_dict_Consider[\"6_Zendesk_Not_Worked\"]=[0, 0]\n",
    "grid_dict_Consider[\"7_Zendesk_Worked_New_Open\"]=[0, 1] \n",
    "grid_dict_Consider[\"8_Live_Channel_Single_Contact\"]=[0, 1] \n",
    "for j in range(0,2):\n",
    "    df_array[j]['Consider'] = np.nan\n",
    "    Mask = (df_array[j]['Multiple_Contacts'] == 1) & (df_array[j]['Multiple_Contacts (Thres Adjusted)']== 0) \n",
    "    Converted_DF = pd.DataFrame(df_array[j][\"Final_Category\"].map(grid_dict_Consider).values.tolist())\n",
    "    Consider_Only = Converted_DF.iloc[:,1]\n",
    "\n",
    "    df_array[j]['Consider'] = Consider_Only\n",
    "    df_array[j].loc[(df_array[j]['Final_Category'] == \"0_Multiple_Contacts\") & (Mask == True), 'Consider'] = 0\n",
    "    df_array[j].loc[(df_array[j]['Final_Category'] == \"0_Multiple_Contacts\") & (Mask == False), 'Consider'] = 1\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "  \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reopen Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,2):\n",
    "    Converted_DF = pd.DataFrame(df_array[j][\"Final_Category\"].map(grid_dict_Consider).values.tolist())\n",
    "    Reopen_Only = Converted_DF.iloc[:,0]\n",
    "    df_array[j].loc[(df_array[j]['Final_Category'] == \"0_Multiple_Contacts\") & (df_array[j]['Consider'] == 0), 'Reopen'] = 0\n",
    "    df_array[j].loc[(df_array[j]['Final_Category'] == \"0_Multiple_Contacts\") & (df_array[j]['Consider'] == 1), 'Reopen'] = df_array[j]['Multiple_Contacts (Thres Adjusted)']\n",
    "    df_array[j].loc[(df_array[j]['Final_Category'] != \"0_Multiple_Contacts\"), 'Reopen'] = Reopen_Only\n",
    "    # df_array[j]['Reopen']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact_Count Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,2):\n",
    "    df_array[j]['Contact_Count']=((df_array[j]['Multiple_Contacts'] == 1) & (df_array[j]['Multiple_Contacts (Thres Adjusted)'] == 0)).apply(lambda x: 1 if x == False else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final_Reopen Column Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,2):\n",
    "    Mask_0_contact = df_array[j]['Contact_Count'] == 0\n",
    "    Mask_Disposition = pd.isnull(df_array[j]['Disposition_Category']) == False\n",
    "    df_array[j]['Final_Reopen'] = np.nan\n",
    "    df_array[j]['Final_Reopen'].loc[Mask_0_contact == True] = 0\n",
    "    df_array[j].loc[(Mask_0_contact == False) & (Mask_Disposition == True), 'Final_Reopen'] = 1\n",
    "    df_array[j].loc[(Mask_0_contact == False) & (Mask_Disposition == False), 'Final_Reopen'] = df_array[j]['Multiple_Contacts (Thres Adjusted)'] \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Customer and Captain files with all Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_array[0].to_excel('C:/Users/momen.jamil/Desktop/Careem/Work/Lovish/FCR/Week 48/Validation/Customer_Code_Result.xlsx', index =False , sheet_name='Sheet1')\n",
    "# df_array[1].to_excel('C:/Users/momen.jamil/Desktop/Careem/Work/Lovish/FCR/Week 48/Validation/Captain_Code_Result.xlsx', index =False , sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Customer and Captain FCR Results to Excel file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_Group = df_array[0].groupby('media')\n",
    "country_Series = country_Group['Reopen'].sum() / country_Group['Consider'].sum()\n",
    "FCR = country_Series*100\n",
    "writer = pd.ExcelWriter('C:/Users/momen.jamil/Desktop/Careem/Work/Lovish/FCR/Week 48/Validation/FCR_Results.xlsx', engine='xlsxwriter')\n",
    "FCR.to_excel(writer, sheet_name='Customer_FCR', index=True)\n",
    "country_Group = df_array[1].groupby('media')\n",
    "country_Series = country_Group['Reopen'].sum() / country_Group['Consider'].sum()\n",
    "FCR = country_Series*100\n",
    "FCR.to_excel(writer, sheet_name='Captain_FCR', index=True)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
